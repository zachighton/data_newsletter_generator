{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newsletter Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newspaper and basic imports\n",
    "import newspaper \n",
    "from newspaper import Article\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Zac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Zac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk imports\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.text import Text\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image imports\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping the working from slice of dataframe from coming up\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summariser imports\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export imports\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather import\n",
    "from pyowm.owm import OWM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock inports\n",
    "import pandas_datareader as pdr\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of websites to scrape\n",
    "\n",
    "websites = ['https://medium.com/tag/technology', 'https://towardsdatascience.com', 'https://python.plainenglish.io/', 'https://www.kdnuggets.com', 'https://www.dataversity.net', 'https://www.ibm.com/blogs/journey-to-ai/', 'https://insidebigdata.com', 'https://www.datarobot.com/blog/', 'https://dataaspirant.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to pull articles from websites:\n",
    "\n",
    "def websites_pull(websites, no_articles):\n",
    "\n",
    "    # lists for data to go into\n",
    "\n",
    "    website = []\n",
    "    title = []\n",
    "    body = []\n",
    "    authors = []\n",
    "    top_image = []\n",
    "    keywords = []\n",
    "        \n",
    "    # scraping websites\n",
    "\n",
    "    for url in websites:\n",
    "\n",
    "        paper = newspaper.build(url, memoize_articles=False)\n",
    "        \n",
    "        paper_articles = []\n",
    "\n",
    "        print(url)\n",
    "\n",
    "        for article in paper.articles[1:30]:\n",
    "                if '#comments' not in article.url:\n",
    "                    paper_articles.append(article.url)\n",
    "\n",
    "        if len(paper_articles) > no_articles:\n",
    "\n",
    "            for i in tqdm(range(no_articles)):\n",
    "\n",
    "                try:\n",
    "\n",
    "                    article = Article(paper_articles[i], language=\"en\")\n",
    "\n",
    "                    article.download()\n",
    "                    article.parse()\n",
    "\n",
    "                    website.append(article.url)\n",
    "\n",
    "                    title.append(article.title)\n",
    "\n",
    "                    body.append(article.text)\n",
    "\n",
    "                    authors.append(article.authors)\n",
    "\n",
    "                    top_image.append(article.top_image)\n",
    "\n",
    "                    article.nlp()\n",
    "\n",
    "                    keywords.append(article.keywords)\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        else:\n",
    "            for i in tqdm(range(len(paper_articles))):\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        article = Article(paper_articles[i], language=\"en\")\n",
    "\n",
    "                        article.download()\n",
    "                        article.parse()\n",
    "\n",
    "                        website.append(article.url)\n",
    "\n",
    "                        title.append(article.title)\n",
    "\n",
    "                        body.append(article.text)\n",
    "\n",
    "                        authors.append(article.authors)\n",
    "\n",
    "                        top_image.append(article.top_image)\n",
    "\n",
    "                        article.nlp()\n",
    "\n",
    "                        keywords.append(article.keywords) \n",
    "                    \n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "    # putting all the data in a dataframe\n",
    "\n",
    "    df = pd.DataFrame({'title':title, 'authors':authors, 'body':body, 'website':website,\n",
    "    'image':top_image, 'keywords':keywords})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://medium.com/tag/technology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://towardsdatascience.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:10<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://python.plainenglish.io/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:13<00:00,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.kdnuggets.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.dataversity.net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:09<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ibm.com/blogs/journey-to-ai/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://insidebigdata.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.datarobot.com/blog/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://dataaspirant.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# put in a list of websites and the number of articles desired\n",
    "# returns a dataframe of articles\n",
    "\n",
    "df = websites_pull(websites, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting rid of duplicate articles\n",
    "df['title'] = df['title'].drop_duplicates()\n",
    "df = df.dropna()\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the main text - removing \\n\n",
    "df['body'] = df['body'].apply(lambda x:x.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing double spaces\n",
    "df['body'] = df['body'].apply(lambda x:' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning urls\n",
    "df['website']=df['website'].apply(lambda x:x.split('?')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping short articles - sometimes not all the article is scraped\n",
    "df.drop(df[df['body'].str.len() < 1000].index, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop articles which start with a number (summariser doesn't deal well with e.g. 7 steps to...) \n",
    "# if number is not 20 - just to keep 2021 and 2022 articles in the mix\n",
    "df.drop(df[(df['title'].str[0].str.isdigit()) & (df['title'].str[0:2] != '20')].index, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bringing back most frequent word pairs found in each article\n",
    "\n",
    "col_final = []\n",
    "\n",
    "for i in range(len(df['body'])):\n",
    "\n",
    "    textfile = df['body'][i]\n",
    "\n",
    "    # getting tokens\n",
    "    tokens = word_tokenize(textfile)\n",
    "\n",
    "    # making everying lower case\n",
    "    lower_tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # getting rid of numbers\n",
    "    clean1 = [word for word in lower_tokens if word.isalpha()]\n",
    "    \n",
    "    # getting stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    word_m_stop = [word for word in clean1 if not word in stop_words]\n",
    "    final_text = Text(word_m_stop)\n",
    "\n",
    "    # adding collocations to list\n",
    "    col_list = final_text.collocation_list()\n",
    "\n",
    "    col_list = [list(i) for i in col_list]\n",
    "\n",
    "    col_med = []\n",
    "\n",
    "    for i in range(len(col_list)):\n",
    "        sentence = col_list[i]\n",
    "        col_med.append(' '.join(sentence))\n",
    "        \n",
    "    col_final.append(col_med)\n",
    "    \n",
    "# creating dataframe column\n",
    "\n",
    "df['collocations'] = col_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering for Relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data science key terms\n",
    "filter_words_upper = ['Artificial Intelligence',\n",
    " 'Big Data', ''\n",
    " 'Clustering',\n",
    " 'Python',\n",
    " 'Outlier',\n",
    " 'Data Science',\n",
    " 'Data Warehouse',\n",
    " 'Machine Learning',\n",
    " 'Artificial Intelligence',\n",
    " 'Data Analysis',\n",
    " 'Data Engineering',\n",
    " 'Data Visualization',\n",
    " 'Data Wrangling',\n",
    " 'Box Plot',\n",
    " 'Correlation',\n",
    " 'dashboard',\n",
    " 'EDA',\n",
    " 'Histogram',\n",
    " 'Hypothesis',\n",
    " 'Iteration',\n",
    " 'AWS',\n",
    " 'azure',\n",
    " 'Numpy',\n",
    " 'pandas',\n",
    " 'matplotlib',\n",
    " 'seaborn',\n",
    " 'Bayes Theorem',\n",
    " 'Decision Tree',\n",
    " 'Quantile',\n",
    " 'Predictive Modelling',\n",
    " 'Standard Deviation',\n",
    " 'Random Forest',\n",
    " 'boolean',\n",
    " 'Fuzzy Logic',\n",
    " 'Regression',\n",
    " 'Classification',\n",
    " 'Overfit',\n",
    " 'underfit',\n",
    " 'Statistical Significance',\n",
    " 'Variance',\n",
    " 'Deep Learning',\n",
    " 'Feature Selection',\n",
    " 'Supervised Machine Learning',\n",
    " 'Unsupervised Machine Learning',\n",
    " 'Binary Variable',\n",
    " 'Binomial Distribution',\n",
    " 'Computer Vision',\n",
    " 'Confusion Matrix',\n",
    " 'covariance',\n",
    " 'Degrees of Freedom',\n",
    " 'Evaluation Metrics',\n",
    " 'F-Score',\n",
    " 'Hadoop',\n",
    " 'Hyperparameter',\n",
    " 'IQR',\n",
    " 'Keras',\n",
    " 'kNN',\n",
    " 'NoSQL',\n",
    " 'Normal Distribution',\n",
    " 'Normalize',\n",
    " 'One Hot Encoding',\n",
    " 'dummies',\n",
    " 'recall',\n",
    " 'P-Value',\n",
    " 'roc',\n",
    " 'auc',\n",
    " 'Root Mean Squared Error',\n",
    " 'rmse',\n",
    " 'Skewness',\n",
    " 'SMOTE',\n",
    " 'stadardize',\n",
    " 'Standard error',\n",
    " 'TensorFlow',\n",
    " 'Univariate Analysis',\n",
    " 'Z-test',\n",
    " 'Residual',\n",
    " 'Neural Network',\n",
    " 'Autoregression',\n",
    " 'Backpropogation',\n",
    " 'Bagging',\n",
    " 'Bias-Variance Trade-off',\n",
    " 'Boosting',\n",
    " 'Bootstrapping',\n",
    " 'Classification Threshold',\n",
    " 'Convex Function',\n",
    " 'Cosine Similarity',\n",
    " 'Cost Function',\n",
    " 'Cross Entropy',\n",
    " 'Cross Validation',\n",
    " 'DBScan',\n",
    " 'Decision Boundary',\n",
    " 'Dplyr',\n",
    " 'Early Stopping',\n",
    " 'Feature Hashing',\n",
    " 'Gated Recurrent Unit',\n",
    " 'Hidden Markov Model',\n",
    " 'Hierarchical Clustering',\n",
    " 'Holdout Sample',\n",
    " 'Holt-Winters Forecasting',\n",
    " 'Imputation',\n",
    " 'K-Means',\n",
    " 'Kurtosis',\n",
    " 'Lasso',\n",
    " 'Maximum Likelihood Estimation',\n",
    " 'Multivariate Analysis',\n",
    " 'Naive Bayes',\n",
    " 'Polynomial Regression',\n",
    " 'Ridge Regression',\n",
    " 'Rotational Invariance',\n",
    " ' Semi-Supervised Learning',\n",
    " 'Stochastic Gradient Descent',\n",
    " 'SVM', \n",
    " 'BART', \n",
    " 'BERT',\n",
    " 'NLP',\n",
    " 'Pandas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making filtered words lower\n",
    "filter_words = [word.lower() for word in filter_words_upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating masks - getting boolean arrays for where keywords, collocations or title \n",
    "# contain filter words\n",
    "mask = np.array([bool(set(map(str, x)) & set(filter_words)) for x in df['collocations']])\n",
    "mask2 = np.array([bool(set(map(str, x)) & set(filter_words)) for x in df['keywords']])\n",
    "mask3 = np.array([bool(set(map(str, x)) & set(filter_words)) for x in df['title']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering by mask and resetting index\n",
    "df_filtered = df[mask | mask2 | mask3]\n",
    "df_filtered.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging articles - beginner, intermediate, advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting lower case tag words\n",
    "\n",
    "# difficulty key words\n",
    "beginner_upper = ['Artificial Intelligence', 'Big Data', 'Python', 'Outlier', 'Data Science', 'Data Warehouse', 'Machine Learning', 'Artificial Intelligence', 'Data Analysis', 'Data Engineering', 'Data Visualization', 'Data Wrangling', 'Box Plot', 'Correlation', 'dashboard', 'EDA', 'Histogram', 'Hypothesis', 'Iteration', 'AWS', 'azure', 'Numpy', 'pandas', 'matplotlib', 'seaborn']\n",
    "\n",
    "beginner = [word.lower() for word in beginner_upper]\n",
    "\n",
    "medium_upper = ['Clustering','Bayes Theorem', 'Decision Tree', 'Quantile', 'Predictive Modelling', 'Standard Deviation', 'Random Forest', 'boolean', 'Fuzzy Logic', 'Regression', 'Classification', 'Overfit', 'underfit', 'Statistical Significance', 'Variance', 'Deep Learning', 'Feature Selection', 'Supervised Machine Learning', 'Unsupervised Machine Learning', 'Binary Variable', 'Binomial Distribution', 'Computer Vision', 'Confusion Matrix', 'covariance', 'Degrees of Freedom', 'Evaluation Metrics', 'F-Score', 'Hadoop', 'Hyperparameter', 'IQR', 'Keras', 'kNN', 'NoSQL', 'Normal Distribution', 'Normalize', 'One Hot Encoding', 'dummies', 'recall', 'P-Value', 'roc', 'auc', 'Root Mean Squared Error', 'rmse', 'Skewness', 'SMOTE', 'standardize', 'Standard error', 'TensorFlow', 'Univariate Analysis', 'Z-test']\n",
    "\n",
    "medium = [word.lower() for word in medium_upper]\n",
    "\n",
    "advanced_upper = ['Residual', 'Neural Network', 'Autoregression', 'Backpropogation', 'Bagging', 'Bias-Variance Trade-off', 'Boosting', 'Bootstrapping', 'Classification Threshold', 'Convex Function', 'Cosine Similarity', 'Cost Function', 'Cross Entropy', 'Cross Validation', 'DBScan', 'Decision Boundary', 'Dplyr', 'Early Stopping', 'Feature Hashing', 'Gated Recurrent Unit', 'Hidden Markov Model', 'Hierarchical Clustering', 'Holdout Sample', 'Holt-Winters Forecasting', 'Imputation', 'K-Means', 'Kurtosis', 'Lasso', 'Maximum Likelihood Estimation', 'Multivariate Analysis', 'Naive Bayes', 'Polynomial Regression', 'Ridge Regression', 'Rotational Invariance', ' Semi-Supervised Learning', 'Stochastic Gradient Descent', 'SVM']\n",
    "\n",
    "advanced = [word.lower() for word in advanced_upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the percentage of keywords which are beginner, intermediate and advanced\n",
    "\n",
    "beg = []\n",
    "med = []\n",
    "adv = []\n",
    "\n",
    "\n",
    "for i in range(len(df_filtered['body'])):\n",
    "\n",
    "    beg_count = 0\n",
    "\n",
    "    for word in beginner:\n",
    "        if word in df_filtered['body'][i]:\n",
    "            beg_count +=1\n",
    "\n",
    "    med_count = 0\n",
    "\n",
    "    for word in medium:\n",
    "        if word in df_filtered['body'][i]:\n",
    "            med_count +=1\n",
    "\n",
    "    adv_count = 0\n",
    "\n",
    "    for word in advanced:\n",
    "        if word in df_filtered['body'][i]:\n",
    "            adv_count +=1\n",
    "\n",
    "    total_count = beg_count + med_count + adv_count\n",
    "\n",
    "    if total_count != 0:\n",
    "\n",
    "        beg_percentage = beg_count/total_count\n",
    "        med_percentage = med_count/total_count\n",
    "        adv_percentage = adv_count/total_count\n",
    "\n",
    "        beg.append(beg_percentage)\n",
    "        med.append(med_percentage)\n",
    "        adv.append(adv_percentage)\n",
    "\n",
    "    else:\n",
    "        beg.append(0)\n",
    "        med.append(0)\n",
    "        adv.append(0)\n",
    "\n",
    "# adding columns to dataframe\n",
    "\n",
    "df_filtered['percentage_beginner'] = beg\n",
    "df_filtered['percentage_medium'] = med\n",
    "df_filtered['percentage_advanced'] = adv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting back difficulty tag column\n",
    "# adjusted advanced threshold\n",
    "\n",
    "tags = []\n",
    "\n",
    "for i in range(len(df_filtered)):\n",
    "    if df_filtered['percentage_advanced'][i] >= 0.3:\n",
    "        tags.append('Advanced')\n",
    "    elif df_filtered['percentage_medium'][i] >= 0.5:\n",
    "        tags.append('Intermediate')\n",
    "    else:\n",
    "        tags.append('Beginner')\n",
    "\n",
    "df_filtered['tag'] = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>body</th>\n",
       "      <th>website</th>\n",
       "      <th>image</th>\n",
       "      <th>keywords</th>\n",
       "      <th>collocations</th>\n",
       "      <th>percentage_beginner</th>\n",
       "      <th>percentage_medium</th>\n",
       "      <th>percentage_advanced</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I Took 12 Data Science Courses For 3 Months — ...</td>\n",
       "      <td>[Benjamin Nweke]</td>\n",
       "      <td>1. Keep your mind open to limitless possibilit...</td>\n",
       "      <td>https://towardsdatascience.com/i-took-12-data-...</td>\n",
       "      <td>https://miro.medium.com/max/1200/1*8pPrgUYTar9...</td>\n",
       "      <td>[earlier, took, heres, 12, scientists, wish, p...</td>\n",
       "      <td>[data science, data scientists, already prior,...</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Beginner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Python: Short Introduction to os.path Module</td>\n",
       "      <td>[Tony Li Xu]</td>\n",
       "      <td>Python: Short Introduction to os.path Module o...</td>\n",
       "      <td>https://python.plainenglish.io/python-short-in...</td>\n",
       "      <td>https://miro.medium.com/max/543/0*Vih-j6psgmaX...</td>\n",
       "      <td>[tail, returns, file, ospath, module, short, p...</td>\n",
       "      <td>[path print, print path, true path, commonly u...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Beginner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to Predict Customer Lifetime Value (CLV) i...</td>\n",
       "      <td>[Muhammed Resit Cicekdag]</td>\n",
       "      <td>How to Predict Customer Lifetime Value (CLV) i...</td>\n",
       "      <td>https://python.plainenglish.io/how-to-predict-...</td>\n",
       "      <td>https://miro.medium.com/max/1200/0*MrS-82Gy9Tf...</td>\n",
       "      <td>[values, lets, bgnbd, number, up_limit, python...</td>\n",
       "      <td>[dataframe variable, uniquely assigned, variab...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Beginner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Best Practices for MLOps Documentation</td>\n",
       "      <td>[]</td>\n",
       "      <td>Whether it's an ML side project or adding a ne...</td>\n",
       "      <td>https://www.kdnuggets.com/2021/12/best-practic...</td>\n",
       "      <td>https://www.kdnuggets.com/wp-content/uploads/b...</td>\n",
       "      <td>[best, practices, technical, machine, automati...</td>\n",
       "      <td>[machine learning, technical documentation, ml...</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Beginner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Write Clean Python Code Using Pipes</td>\n",
       "      <td>[]</td>\n",
       "      <td>By Khuyen Tran, Data Science Intern Motivation...</td>\n",
       "      <td>https://www.kdnuggets.com/2021/12/write-clean-...</td>\n",
       "      <td>https://www.kdnuggets.com/wp-content/uploads/w...</td>\n",
       "      <td>[values, write, list, clean, elements, python,...</td>\n",
       "      <td>[image author, data science, unfold iterables,...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Beginner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  I Took 12 Data Science Courses For 3 Months — ...   \n",
       "1       Python: Short Introduction to os.path Module   \n",
       "2  How to Predict Customer Lifetime Value (CLV) i...   \n",
       "3             Best Practices for MLOps Documentation   \n",
       "4                Write Clean Python Code Using Pipes   \n",
       "\n",
       "                     authors  \\\n",
       "0           [Benjamin Nweke]   \n",
       "1               [Tony Li Xu]   \n",
       "2  [Muhammed Resit Cicekdag]   \n",
       "3                         []   \n",
       "4                         []   \n",
       "\n",
       "                                                body  \\\n",
       "0  1. Keep your mind open to limitless possibilit...   \n",
       "1  Python: Short Introduction to os.path Module o...   \n",
       "2  How to Predict Customer Lifetime Value (CLV) i...   \n",
       "3  Whether it's an ML side project or adding a ne...   \n",
       "4  By Khuyen Tran, Data Science Intern Motivation...   \n",
       "\n",
       "                                             website  \\\n",
       "0  https://towardsdatascience.com/i-took-12-data-...   \n",
       "1  https://python.plainenglish.io/python-short-in...   \n",
       "2  https://python.plainenglish.io/how-to-predict-...   \n",
       "3  https://www.kdnuggets.com/2021/12/best-practic...   \n",
       "4  https://www.kdnuggets.com/2021/12/write-clean-...   \n",
       "\n",
       "                                               image  \\\n",
       "0  https://miro.medium.com/max/1200/1*8pPrgUYTar9...   \n",
       "1  https://miro.medium.com/max/543/0*Vih-j6psgmaX...   \n",
       "2  https://miro.medium.com/max/1200/0*MrS-82Gy9Tf...   \n",
       "3  https://www.kdnuggets.com/wp-content/uploads/b...   \n",
       "4  https://www.kdnuggets.com/wp-content/uploads/w...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [earlier, took, heres, 12, scientists, wish, p...   \n",
       "1  [tail, returns, file, ospath, module, short, p...   \n",
       "2  [values, lets, bgnbd, number, up_limit, python...   \n",
       "3  [best, practices, technical, machine, automati...   \n",
       "4  [values, write, list, clean, elements, python,...   \n",
       "\n",
       "                                        collocations  percentage_beginner  \\\n",
       "0  [data science, data scientists, already prior,...             0.833333   \n",
       "1  [path print, print path, true path, commonly u...             1.000000   \n",
       "2  [dataframe variable, uniquely assigned, variab...             0.666667   \n",
       "3  [machine learning, technical documentation, ml...             0.571429   \n",
       "4  [image author, data science, unfold iterables,...             1.000000   \n",
       "\n",
       "   percentage_medium  percentage_advanced       tag  \n",
       "0           0.166667                  0.0  Beginner  \n",
       "1           0.000000                  0.0  Beginner  \n",
       "2           0.333333                  0.0  Beginner  \n",
       "3           0.428571                  0.0  Beginner  \n",
       "4           0.000000                  0.0  Beginner  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Read_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting read time\n",
    "# according to the internet a person reads around 238 words per minute\n",
    "import math\n",
    "\n",
    "df_filtered['read_time'] = df_filtered['body'].apply(lambda x:math.ceil(len(x.split())/238))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Articles to Suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure there are 5 articles from different websites\n",
    "# loop until this is true\n",
    "\n",
    "if len(df_filtered) > 5:\n",
    "    df_sample = df_filtered.sample(n=5)\n",
    "    df_sample = df_sample['website'].apply(lambda x:x.split('/')[2]).drop_duplicates()\n",
    "    \n",
    "    while len(df_sample) < 5:\n",
    "        df_sample = df_filtered.sample(n=5)\n",
    "        df_sample['website'] = df_sample['website'].apply(lambda x:x.split('/')[2]).drop_duplicates()\n",
    "        df_sample.dropna(inplace=True)\n",
    "\n",
    "df_filtered = df_filtered.iloc[df_sample.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Summaries - BART Summariser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting model\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate summaries\n",
    "# for more information on BART see README\n",
    "\n",
    "def get_summaries(df_filtered, length):\n",
    "\n",
    "    summaries = []\n",
    "\n",
    "    for i in range(len(df_filtered['body'])):\n",
    "        document = df_filtered['body'][i]\n",
    "\n",
    "        # tokenize text\n",
    "\n",
    "        inputs = tokenizer([document], max_length=1024, return_tensors='pt', truncation=True)\n",
    "\n",
    "        # generate summary\n",
    "\n",
    "        summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=length, early_stopping=True, length_penalty=2)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    # add to dataframe\n",
    "\n",
    "    df_filtered['summary'] = summaries\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dataframe and max length of summary\n",
    "\n",
    "df_filtered = get_summaries(df_filtered, 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cleaning summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing last sentence from summary if not complete\n",
    "\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "summaries_complete = []\n",
    "\n",
    "for i in range(len(df_filtered['summary'])):\n",
    "    string = df_filtered['summary'][i]\n",
    "\n",
    "    sentences = sent_tokenizer.tokenize(string)\n",
    "\n",
    "    add = []\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        if sentences[i][-1] == '.':\n",
    "            add.append(sentences[i])\n",
    "\n",
    "\n",
    "    summaries_complete.append(\" \".join(s for s in add))\n",
    "\n",
    "df_filtered['summary'] = summaries_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing sentences which are the same as the title\n",
    "\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "summaries_no_title = []\n",
    "\n",
    "for i in range(len(df_filtered['summary'])):\n",
    "    string = df_filtered['summary'][i]\n",
    "\n",
    "    sentences = sent_tokenizer.tokenize(string)\n",
    "\n",
    "    add = []\n",
    "\n",
    "    for s in range(len(sentences)):\n",
    "        if sentences[s][:-1] != df_filtered['title'][i]:\n",
    "            add.append(sentences[s])\n",
    "\n",
    "    summaries_no_title.append(\" \".join(s for s in add))\n",
    "\n",
    "df_filtered['summary'] = summaries_no_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing any white space from start and end of summaries\n",
    "df_filtered['summary'] = df_filtered['summary'].apply(lambda x:x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newspaper Function - exports article summaries and information, gets weather and stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsletter():\n",
    "\n",
    "    # make a folder to put contents with todays date\n",
    "\n",
    "    path = '/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/'\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "\n",
    "    os.mkdir(path)\n",
    "\n",
    "    with open(os.path.join('/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/','Summaries.txt'), \"w\") as f:\n",
    "        f.write(datetime.strftime(datetime.now(), '%A %d %B') + '\\n\\n')\n",
    "\n",
    "    # make folder for images\n",
    "\n",
    "    path = '/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/Images/'\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "        \n",
    "    os.mkdir(path)\n",
    "\n",
    "    for i in range(len(df_filtered)):\n",
    "\n",
    "        # add text information including summary\n",
    "\n",
    "        with open('/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/Summaries.txt', 'a') as f:\n",
    "            f.write('Tag: ' + df_filtered['tag'][i].upper() + ' - Read Time: ' + str(df_filtered['read_time'][i]) + '\\n\\n' + df_filtered['title'][i] + '\\n\\n' + df_filtered['summary'][i] + '\\n\\n' + 'Read the full story here: ' + df_filtered['website'][i] + '\\n\\n' + '---------------' + '\\n\\n')\n",
    "\n",
    "        # add images\n",
    "\n",
    "        try:\n",
    "            response = requests.get(df_filtered['image'][i])\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            img.save('/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/Images/' + df_filtered['title'][i] +'.png')\n",
    "\n",
    "        except:\n",
    "            #print('No image found for:', df_filtered['title'][i])\n",
    "\n",
    "            try:\n",
    "                response = requests.get(df_filtered['image'][i])\n",
    "                img = Image.open(BytesIO(response.content))\n",
    "                img.save('/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/Images/' + df_filtered['title'][i] +'.jpg')\n",
    "            except:\n",
    "               print('No image found for:', df_filtered['title'][i])\n",
    "        continue\n",
    "\n",
    "\n",
    "    # getting weather information\n",
    "\n",
    "    owm = OWM('b33c2d9566ba8b3c3260afc40c91d012')\n",
    "    mgr = owm.weather_manager()\n",
    "    observation = mgr.one_call(lat = 41.38879, lon = 2.15899)\n",
    "    weather = observation.forecast_daily[0]\n",
    "\n",
    "    # saving weather\n",
    "\n",
    "    path = '/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/Weather/'\n",
    "    \n",
    "    # make a folder to put contents\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "\n",
    "    os.mkdir(path)\n",
    "\n",
    "    with open(os.path.join('/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/Weather/' + 'weather.txt'), \"w\") as f:\n",
    "        \n",
    "        f.write('Barcelona'.upper()+ '\\n\\n')\n",
    "\n",
    "        weather_dict = {'clear sky':'Clear Skies', 'few clouds':'Partly Cloudy', 'scattered clouds':'Partly Cloudy', 'broken clouds':'Cloudy', 'shower rain':'Light Showers', 'rain':'Rain', 'thunderstorm':'Stormy', 'snow':'Snow', 'mist':'Misty'}\n",
    "\n",
    "        if weather.detailed_status in weather_dict.keys():\n",
    "\n",
    "            f.write(weather_dict[weather.detailed_status].upper() + '\\n\\n')\n",
    "        \n",
    "        else:\n",
    "            f.write(weather.detailed_status.upper() + '\\n\\n')\n",
    "    \n",
    "        # high and low temp\n",
    "        f.write('Low/High: ' + str(round(weather.temperature('celsius')['min'])) + '°/' + str(round(weather.temperature('celsius')['max']))+ '°\\n\\n')\n",
    "\n",
    "        # sunrise\n",
    "        timestamp = datetime.fromtimestamp(weather.srise_time)\n",
    "        f.write('Sunrise: ' + timestamp.strftime('%H:%M') + '\\n')\n",
    "\n",
    "        # sunset\n",
    "        timestamp = datetime.fromtimestamp(weather.sset_time)\n",
    "        f.write('Sunset: '+ timestamp.strftime('%H:%M'))\n",
    "\n",
    "    # getting back weather icon\n",
    "\n",
    "    try:\n",
    "\n",
    "        url = weather.weather_icon_url()\n",
    "\n",
    "        response = requests.get(url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "\n",
    "        img.save('/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/Weather/' + 'weathericon' + '.png')\n",
    "\n",
    "    except:\n",
    "        print('Couldn\\'t fetch weather icon.')\n",
    "\n",
    "    # stocks\n",
    "\n",
    "    stock_weekdays = ['Tue', 'Wed', 'Thu', 'Fri']\n",
    "\n",
    "    stock_dict = {'^GSPC':'S&P 500', '^IBEX':'IBEX 35', '^IXIC':'NASDAQ', '^DJI':'DOW'}\n",
    "\n",
    "    path = '/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/Stocks/'\n",
    "    \n",
    "    # make a folder to put contents\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "\n",
    "    os.mkdir(path)\n",
    "\n",
    "    with open(os.path.join('/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/Stocks/' + 'stocks.txt'), \"w\") as f:\n",
    "        f.write('Stocks: \\n\\n')\n",
    "\n",
    "    # get daily stock change for weekdays\n",
    "\n",
    "    if datetime.strftime(datetime.now() - timedelta(1), '%a') in stock_weekdays:\n",
    "\n",
    "        for stock_ticker in stock_dict.keys():\n",
    "\n",
    "            df = pdr.get_data_yahoo(stock_ticker)\n",
    "            df = df.reset_index()\n",
    "            yesterday_close = df[df['Date'] == datetime.strftime(datetime.now() - timedelta(1), '%Y-%m-%d')]['Close'].item()\n",
    "\n",
    "            try:\n",
    "                day_before_yesterday_close = df[df['Date'] == datetime.strftime(datetime.now() - timedelta(2), '%Y-%m-%d')]['Close'].item()\n",
    "            except:\n",
    "                print('No stock data available today.')\n",
    "\n",
    "            change = yesterday_close - day_before_yesterday_close\n",
    "            percentage_change = round((change/day_before_yesterday_close)*100, 2)\n",
    "\n",
    "            with open(os.path.join('/Users/Zac/Desktop/Newsletter - ' + datetime.strftime(datetime.now(), '%d-%m-%Y') + '/Stocks/' + 'stocks.txt'), \"a\") as f:\n",
    "\n",
    "                if percentage_change > 0:\n",
    "                \n",
    "                    f.write(stock_dict[stock_ticker] + ' +' + str(percentage_change) +  '%\\n\\n')\n",
    "                \n",
    "                else:\n",
    "                    f.write(stock_dict[stock_ticker]+ ' ' +str(percentage_change) + '%\\n\\n')\n",
    "\n",
    "    # if not a weekday, get yearly change\n",
    "\n",
    "    else:\n",
    "        for stock_ticker in stock_dict.keys():\n",
    "\n",
    "            df = pdr.get_data_yahoo(stock_ticker)\n",
    "            df = df.reset_index()\n",
    "\n",
    "            start_year = df[df['Date'] == datetime.strftime(datetime(datetime.today().year, 2, 1), '%Y-%m-%d')]['Close'].item()\n",
    "            yesterday_close = df[df['Date'] == datetime.strftime(datetime.now() - timedelta(1), '%Y-%m-%d')]['Close'].item()\n",
    "\n",
    "            change = yesterday_close - start_year\n",
    "            percentage_change = round((change/start_year)*100, 2)\n",
    "\n",
    "            if percentage_change > 0:\n",
    "                    \n",
    "                f.write('Yearly Change: ' + stock_dict[stock_ticker] + ' +' + str(percentage_change) +  '%')\n",
    "                    \n",
    "            else:\n",
    "                f.write('Yearly Change: ' + stock_dict[stock_ticker]+ ' ' +str(percentage_change) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No image found for: Customer Analytics and AI: Better Together\n",
      "No image found for: “Above the Trend Line” – Your Industry Rumor Central for 12/10/2021\n",
      "No image found for: Write Clean Python Code Using Pipes\n"
     ]
    }
   ],
   "source": [
    "newsletter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa2d5c2f16a8e749f1d48e9a236e4b14f675e3e0a88517126fa8ace0eecda256"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('da_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
